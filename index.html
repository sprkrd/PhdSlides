<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>New Methods for Bridging Symbolic-Geometric Reasoning, Addressing Uncertainty and Action Learning in Task Planning for Robotics</title>

    <!--<link rel="stylesheet" href="reveal.js/dist/reset.css">-->
    <!--<link rel="stylesheet" href="reveal.js/dist/reveal.css">-->
    <!--<link rel="stylesheet" href="reveal.js/dist/theme/white.css" id="theme">-->

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/reset.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/theme/white.min.css">

    <!-- Theme used for syntax highlighted code -->
    <!--<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">-->

    <link rel="stylesheet" href="css/custom-style.css">
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

        <section class="front-page" data-state="front-page-state">
          <h3>Ph.D. Dissertation</h3>
          <h1>New Methods for Bridging Symbolic-Geometric Reasoning, Addressing Uncertainty and Action Learning in Task Planning for Robotics</h1>

          <hr/>

          <ul class="authors">
            <li>
              <div class="author">
                <span class="name">Alejandro Suárez Hernández</span>
                <span class="email">asuarez@iri.upc.edu</span>
              </div>
            </li>
          </ul>

          <p>Advisors:</p>

          <ul class="authors">
              <li>
                  <div class="author">
                      <span class="name">Carme Torras Genís<sup></sup></span>
                    <span class="email">torras@iri.upc.edu</span>
                  </div>
              </li>
              <li>
                  <div class="author">
                      <span class="name">Guillem Alenyà Ribas<sup></sup></span>
                    <span class="email">galenya@iri.upc.edu</span>
                  </div>
              </li>
          </ul>

          <!--<ol class="affiliations">-->
            <!--<li>Institut de Robòtica i Informàtica Industrial, CSIC-UPC</li>-->
          <!--</ol>-->

          <div class="conference">
            <p class="date">July 25th, 2024</p>
          </div>

          <div class="spaced-horizontally">
              <img src="assets/logos/IRI_logo.svg" height="50px"/>
              <img src="assets/logos/UPC_logo.svg" height="50px"/>
          </div>

          <aside class="notes">

              The title promises three kind of advances: closer coordination
              between task and motion planning, handling uncertainty and
              learning action operators. This is all in hopes of
              making AI planning more widespread in robotics to enjoy its
              advantages. Let's see what those advantages are.

          </aside>

        </section>


        <section>

            <h2 class="numbered-section">Motivation</h2>

            <div class="r-stack">
                <img class="r-stretch" src="img/motivation/frame1.svg">
                <img class="r-stretch fragment" src="img/motivation/frame2.svg">
                <img class="r-stretch fragment" src="img/motivation/frame3.svg">
                <img class="r-stretch fragment" src="img/motivation/frame4.svg">
                <img class="r-stretch fragment" src="img/motivation/frame5.svg">
                <img class="r-stretch fragment" src="img/motivation/frame6.svg">
                <img class="r-stretch fragment" src="img/motivation/frame7.svg">
                <img class="r-stretch fragment" src="img/motivation/frame8.svg">
                <img class="r-stretch fragment" src="img/motivation/frame9.svg">
                <img class="r-stretch fragment" src="img/motivation/frame10.svg">
                <img class="r-stretch fragment" src="img/motivation/frame11.svg">
                <img class="r-stretch fragment" src="img/motivation/frame12.svg">
                <img class="r-stretch fragment" src="img/motivation/frame13.svg">
                <img class="r-stretch fragment" src="img/motivation/frame14.svg">
            </div>


          <aside class="notes">

              <p>
              There're many desirable qualities in robotics: robustness,
              adaptiveness and explainability are a few. Some of them can be
              more easily achieved than others depending on how we choose to
              implement the solution.
              </p>

              <p>
              Let's imagine a spectrum of techniques, with step-by-step
              algorithms at the left hand side, and fully learning approaches
              like RL at the right hand side.
              </p>

              <p>
              Now, fully scripted behaviors have some advantages. They can be
              very robust and they are certainly predictable. However,
              we may code in underperforming assumptions about the task,
              the implementation is very time-consuming for the human operator,
              and the
              solutions cannot adapt to new tasks not seen before by the robot.
              </p>

              <p>
              A few examples of this are assembly lines, tele-operated drones
              and surgical robots.
              </p>

              <p>
              Now let's jump to the other end of the spectrum. Now, making
              a robot learn a task can be less time-consuming for the operator,
              and now the robot can adapt more easily to new tasks. However,
              the robot might not be robust, or at the very least robustness
              cannot be guaranteed on account of the lack of explainability.
              </p>

              <p>
              RL has been used to approach fast-paced tasks such as air hockey.
              We also have the example of AlphaGo beating the leading human
              go player. Finally I'll mention quadruped robots that learn
              or refine their locomotion.
              </p>

              <p>
              In the middle of all this we find task planning, which sort of
              combines the benefits from these two ends. However, robotics
              presents a unique set of obstacles that make the application of
              task planning much more challenging than virtual environments.
              </p>

              <p>
              A few examples of task planning: videogame AI, web service
              composition, and for the sake of giving an embodied agent as an
              example: the Hubble telescope, which uses task planning for
              scheduling observations.
              </p>

              <p>
              Our motivation is to address some of the challenges that
              task planning has to overcome in order to be  more appealing for
              roboticists.
              </p>


          </aside>

        </section>

        <section>
            <h2 class="numbered-section">Objectives</h2>
            <div class="r-stack">
                <img class="r-stretch" src="img/objectives/frame1.svg">
                <img style="z-index:1;position:fixed;top:180px;left:578px" src="img/objectives/lever_example.gif">
                <img style="z-index:1;position:fixed;top:398px;left:275px" class="fragment" src="img/objectives/successful-lever.gif" data-fragment-index="1">
                <img style="z-index:1;position:fixed;top:524px;left:275px" class="fragment" src="img/objectives/unsuccessful-lever.gif" data-fragment-index="1">
                <img class="r-stretch fragment" src="img/objectives/frame2.svg" data-fragment-index="1">
                <img class="r-stretch fragment" src="img/objectives/frame3.svg">
                <img class="r-stretch fragment" src="img/objectives/frame4.svg">
            </div>

            <aside class="notes">
                <p>
                What are our objectives, specifically? Well, first of all, there
                is a gap between symbolic and geometric reasoning that needs to
                be addressed. This gap is usually not a problem in virtual
                applications, but in robotics we have to take into consideration
                that, at the end of the day, actions have to be executed in the
                real world, and the planning of trajectories seldom is independent
                from the abstract set of actions.
                </p>

                <p>
                Secondly, more often than not actions may have unintended effects.
                We would like to anticipate them. This includes avoiding
                risking actions or recovering from them.
                </p>

                <p>
                Another form of uncertainty that we want to address is that that
                comes from partial observability. Many times, the limitations
                of the sensors don't allow the robot to have all the relevant
                information about the world.
                </p>

                <p>
                We must take into account that, in order for task planning to be
                an opriton at all. Therefore, the final objective is to endow
                robots with the ability to
                learn new skills with the least amount of effort from the operator,
                and as naturally as possible.
                </p>
            </aside>
        </section>

        <section>
            <h2 class="numbered-section">Contributions</h2>

            <div class="r-stack">
                <img class="r-stretch" src="img/contributions/frame1.svg">
                <img class="fragment r-stretch" src="img/contributions/frame2.svg">
            </div>

            <aside class="notes">
                <p>
                This is the big picture of all the contributions I've made. For
                now, I don't want to go into detail with them, it's just for you
                to start becoming familiar.
                </p>

                <p>
                I do want that the contributions can be divided into two categories.
                The first one deals with the challenges of planning in the real
                world when a model is given.
                </p>

                <p>
                The second one revolves around acquiring those models from
                demonstration.
                </p>
            </aside>



            <!--<div class="r-hstack" style="justify-content:space-evenly;">-->
                <!--<figure style="width:500px;" class="fragment">-->
                    <!--<img class="" src="img/contributions/first_part.png">-->
                    <!--<figcaption>New planning algorithms</figcaption>-->
                <!--</figure>-->

                <!--<figure style="width:500px;" class="fragment">-->
                    <!--<img class="" src="img/contributions/second_part.png">-->
                    <!--<figcaption>Learning algorithms</figcaption>-->
                <!--</figure>-->
            <!--</div>-->

        </section>

        <!--<section>-->
            <!--<div class="r-stack">-->
                <!--<img class="r-stretch" src="img/contributions/frame1.svg">-->
                <!--<img class="fragment r-stretch" src="img/contributions/frame2.svg">-->
            <!--</div>-->
        <!--</section>-->

        <section>
            <h2 class="numbered-section">Planning and Execution</h2>
            <div class="r-stack">
                <img class="r-stretch" src="img/contributions/splash1.svg">
                <figure class="fragment" style="padding:10px;position:fixed;left:690px;top:120px;background-color:white;border: black dashed">
                    <img style="height:500px;" src="img/contributions/projects1.png">
                    <figcaption>Manipulation use cases</figcaption>
                </figure>
            </div>

            <aside class="notes">

                Let's jump right into planning and execution. At this
                point, I'd like to mention that these contributions have been
                exemplified with manipulation tasks involving one or more
                robot arms. In particular, one of the use cases comes from
                IMAGINE, an European H2020 research programme whose scientific
                goal was to enable robots to understand their surroundings.
                This scientific vision was channeled through the particular use
                case of electromechanical device disassembly.


            </aside>
        </section>

        <section>
            <h3 class="numbered-section">Dual-arm Symbolic-motion Planning</h3>

            <div class="r-hstack">

                <div style="width:50%">
                    <img src="img/contributions/splash11.svg">
                </div>

                <div style="width:50%;" class="fragment">
                    <figure style="">
                      <img width="200" src="img/dual-arm-planning/game-introduction.gif">
                      <figcaption>Complex manipulation...</figcaption>
                    </figure>

                    <figure style="">
                      <img width="200" src="img/dual-arm-planning/picker-and-catcher.png">
                      <figcaption>... with dual-arm robot set-ups</figcaption>
                    </figure>
                </div>

                <aside class="notes">
                    In the first contribution, we wanted
                    dual-arm robots to perform a complex manipulation task, avoiding
                    self collision and dealing with noisy perceptions. We proposed
                    this task, which consists of inserting pieces in a sphere with
                    cavities, as the motivating example. The reason why is that
                    this task has many challenges/opportunities that are representative
                    of the difficulties we often face in robotics.
                </aside>

            </div>


            <div class="reference" style="padding-top: 50px;" >
                <strong>A. Suárez-Hernández</strong>, G. Alenyà, and C. Torras. "Interleaving hierarchical task planning and motion constraint testing for
                dual-arm manipulation." In <strong>IEEE/RSJ IROS 2018</strong>, pp. 4061–4066.
            </div>
        </section>


        <section>
            <h4>Addressed problems</h4>

            <div class="r-hstack">

                <div style="width:50%;">

                    <ul>
                        <li class="fragment semi-fade-out" data-fragment-index="1">Symbolic-geometric reasoning (<strong>O1</strong>)</li>
                        <li class="fragment" data-fragment-index="1">Partial observability (<strong>O3</strong>)</li>
                    </ul>

                </div>

                <div class="r-stack" style="width:50%">
                    <figure class="fragment fade-out" data-fragment-index="1">
                      <img style="height:300px;" src="img/dual-arm-planning/example-symbolic.png">
                      <img style="height:300px;" src="img/dual-arm-planning/example-geometric.png">
                    </figure>
                    <img class="fragment" data-fragment-index="1" style="width:375px;" src="img/dual-arm-planning/example-partial-observability.png">
                </div>

                <aside class="notes">
                    The first of these challenges is that we want to avoid self-collision between the robots.
                    But, at the same time we would like to perform the task as quickly as possible. Therefore,
                    solutions that are too conservative like, for example, always moving one robot
                    at a time and making each robot adopt
                    a home or neutral position each time an action is performed. Planning, thus, becomes necessary.

                    The second one is that the scene is mounted on the ceiling,
                    so it's quite difficult to identify correctly the pieces
                    that are placed on top of the table.
                </aside>
            </div>

        </section>

        <section>
            <h4>Literature</h4>

            <div class="r-stack">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/dual-arm-planning/literature1.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/dual-arm-planning/literature2.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/dual-arm-planning/literature3.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/dual-arm-planning/literature4.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/dual-arm-planning/literature5.svg">
            </div>

            <aside class="notes">

                <p>
                What's usually done? Well, this is the paper we took inspiration from. In it,
                Lozano-Pérez and Kaelbling propose a hierarchical decomposition of the goal in order to keep
                the planning horizon short, they also quantify uncertainty and plan noise reduction actions.
                However, they do not consider the case of self collision nor the problem of
                misidentificating objects.
                </p>

                <p>
                It is worth mentioning that, although the problem of task and motion integration has
                been known for quite some time, it was around the time I worked on this contribution
                that research on this topic surged and became known as TAMP, Task and Motion
                Planning. Reviews like this offer a taxonomy of the available methods.
                </p>

                <p>
                There's been also a surge in tree control structures that have
                the capability to react quickly to disturbances, and apply
                appropriate actions. One such example is Behavior Trees.
                Conversely, we use Hierarchical Task Networks, which are quite
                similar in spirit.
                </p>

                <p>
                And here's further evidence of this trend towards hierarchical
                control trees. This one is pretty similar to Behavior Trees.
                </p>

                <p>
                Finally, I'd like to provide an example of sequential
                strategies. In this case, task planning and motion planning
                were not interleaved, like in our case, and the success of the task
                was entirely depending on a re-planning cycle, which may
                compromise efficiency.
                </p>

            </aside>
        </section>

        <!--<section>-->
            <!--<h4>Specific contributions</h4>-->

            <!--<div class="r-hstack">-->

                <!--<div style="width:50%;">-->

                    <!--<ul>-->
                        <!--<li class="fragment semi-fade-out" data-fragment-index="1">Encode dual-arm alternatives in HTNs (<strong>O1</strong>)</li>-->
                        <!--<li class="fragment" data-fragment-index="1">Quantify uncertainty (<strong>O3</strong>)</li>-->
                    <!--</ul>-->

                <!--</div>-->

                <!--<div class="r-stack" style="width:50%">-->
                  <!--<img class="fragment fade-out" data-fragment-index="1" style="width:100%;" src="img/dual-arm-planning/htn.png">-->
                  <!--<figure style="vertical-align:bottom;" class="fragment" data-fragment-index="1">-->
                    <!--<img style="width:400px;" src="img/dual-arm-planning/dispel-noise.png">-->
                    <!--<img style="width:400px;" src="img/dual-arm-planning/pushing.gif">-->
                    <!--<figcaption>Information gathering (top), reshaping (bottom)</figcaption>-->
                  <!--</figure>-->

                <!--</div>-->
            <!--</div>-->

        <!--</section>-->

        <section data-auto-animate>
            <h4>Specific contributions</h4>

            <div class="r-hstack">

                <div style="width:50%;">

                    <ul>
                        <li class="">Encode dual-arm alternatives in HTNs (<strong>O1</strong>)</li>
                    </ul>

                </div>

                <div class="r-stack" style="width:50%">
                  <img class="" style="width:100%;" src="img/dual-arm-planning/htn.png">
                </div>
            </div>

            <aside class="notes">
                <p>
                The first specific contribution inside this bigger contribution is how we interleave symbolic and motion planning.
                We use task decomposition via Hierarchical Task Networks to divide the task of picking a piece from the
                table and inserting it in the sphere, and this HTN encodes different alternatives for adopting robot
                poses, checking the relevant geometric constraints along the way instead of all at once.
                </p>

            </aside>

        </section>

        <section data-auto-animate>
            <h4>Specific contributions</h4>

            <div class="r-hstack">

                <div style="width:50%;"></div>

                <img class="r-stretch" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn1.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn2.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn3.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn4.png">
                <!--<img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn5.png">-->
                <!--<img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn6.png">-->
                <!--<img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn7.png">-->
            </div>

            <aside class="notes">
                <p>Let's see an example. Imagine that one arm has picked a piece from
                the table and that what remains is inserting it in the sphere.
                The arm that has the sphere attached has to adopt the pose that
                has the relevant cavity facing up, while the arm that has grasped
                the piece needs to adopt the ungrasp position on top of the sphere.
                </p>

                <p>
                What the HTN does is: if the robots are already at the required
                position, nothing else is done in this branch. Otherwise, it tries
                to move both arms at the same time to the requested position,
                invoking the motion planner. If the motion planner does not find a solution,
                we jump to the next alternative, which is to conservatively move the
                robots to their home position, and then move them to the requested position.
                </p>

                <p>
                Notice that here the motion planner is called as needed, during the
                symbolic planning process, without any re-plan needed.
                </p>
            </aside>

        </section>

        <section data-auto-animate>
            <h4>Specific contributions</h4>

            <div class="r-hstack">

                <div style="width:50%;">

                    <ul>
                        <li style="opacity:50%;">Encode dual-arm alternatives in HTNs (<strong>O1</strong>)</li>
                        <li >Quantify uncertainty (<strong>O3</strong>) and preparatory actions</li>
                    </ul>

                </div>

                <div class="r-stack" style="width:50%">
                  <img class="" style="width:100%;" src="img/dual-arm-planning/htn.png">
                </div>
            </div>

            <aside class="notes">
            <p>
            The second big advance of this contribution is how we quantify uncertainty and how the
            uncertainty affects the path expanded by the HTN, and also how geometric criterions can
            trigger preparatory actions.
            </p>
            </aside>

        </section>

        <section data-auto-animate>
            <h4>Specific contributions</h4>

            <div class="r-hstack">

                <div style="width:50%;"></div>

                <img class="r-stretch" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn11.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn12.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn13.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn8.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn9.png">
                <img class="r-stretch fragment fade-in-then-out" style="position:absolute;width:100%;" src="img/dual-arm-planning/htn10.png">
                <figure style="vertical-align:bottom;position:absolute;width:70%;background-color:white;border:black dashed; padding:20px;" class="fragment">
                  <img style="height:200px;" src="img/dual-arm-planning/pushing.gif">
                  <img style="height:200px;" src="img/dual-arm-planning/dispel-noise.png">
                  <figcaption>Reshaping (left), information gathering (right)</figcaption>
                </figure>

            </div>

            <aside class="notes">
            <p>
            In our set-up, pieces are initially on top of table, the camera is at the ceiling
            and it's noisy, so it may be hard to identify correctly the shape of a piece.
            Let's see an example: say we want to insert a piece in the sphere. Before that,
            we have to ensure that we have identified the piece correctly. If the uncertainty
            about a piece shape is low enough, then we're done. Otherwise, an information
            gathering action is triggered. This action consists in bringing the piece closer
            to the camera. One very important difference between this and the work of
            Lozano-Pérez and Kaelbling that I presented earlier is that the subject
            of uncertainty is a critical quality of an object, rather than just its position.
            Therefore, the plan can change, qualitatively: before we thought we were handling
            a piece, but we're handling another.
            </p>

            <p>
            Very similar procedure for reshaping actions. Before picking a piece,
            the robot checks that it is isolated enough so the gripper doesn't collide
            with other pieces in the downwards motion. If not, the planner takes the
            necessary steps to push the offending pieces away.
            </p>
            </aside>

        </section>

        <section>

            <h4>Quantitative experiments</h4>

            <div class="r-hstack" style="justify-content:space-evenly;">
              <figure>
                  <div class="r-stack">
                    <img src="img/dual-arm-planning/results.svg">
                    <img class="fragment" src="img/dual-arm-planning/results1.svg">
                    <img class="fragment fade-in-then-out" src="img/dual-arm-planning/results2.svg">
                    <img class="fragment fade-in-then-out" src="img/dual-arm-planning/results3.svg">
                    <img class="fragment fade-in-then-out" src="img/dual-arm-planning/results4.svg">
                    <img class="fragment fade-in-then-out" src="img/dual-arm-planning/results5.svg">
                    <img class="fragment fade-in-then-out" src="img/dual-arm-planning/results6.svg">
                  </div>
                  <figcaption>Results (simulation)</figcaption>
              </figure>
              <div class="r-vstack">
                <figure class="">
                  <img style="height:240px;" src="img/dual-arm-planning/algorithm-A-scenario-a.gif">
                  <figcaption>Algorithm A in (a)</figcaption>
                </figure>
                <figure class="">
                  <img style="height:240px;" src="img/dual-arm-planning/algorithm-F-scenario-c.gif">
                  <figcaption>Algorithm F in (c)</figcaption>
                </figure>
              </div>
            </div>

            <aside class="notes">
                <p>
                For the results I'm not going to go into much detail. Essentially, I have
                implemented a couple of fully scripted baselines and other planning
                algorithms in different scenarios of varying complexity.
                Algorithm C for instance is planning with information
                gathering actions, but conservatively moving the robots to their home
                position as an intermediate step. Algorithm F is our proposal.
                </p>

                <p>
                The first column of the tables represents planning time, which is
                obviously 0 for the scripted behaviors. Planning algorithm C has
                a very low planning time because it doesn't perform motion planning,
                and our algorithm has a little bit of planning overhead because
                it does perform motion planning to move the two arms cooperatively.
                </p>

                <p>
                The next column is the execution time, very low for all planning
                flavors except C, essentially about the same.
                </p>

                <p>
                The next column is the total time, planning plus execution, which
                you can see is lower for our proposal.
                </p>

                <p>
                This is the reported number of re-plans that had to be triggered,
                it's when a piece has been misidentified and the close examination
                action has helped correct the course of execution.
                </p>

                <p>
                And this is the number of times a piece has been handled incorrectly,
                by trying to insert it in the wrong cavity. This only happens
                in algorithm A because this algorithm always trust the initial state estimation.
                </p>
            </aside>
        </section>

        <section>
            <h4>Qualitative experiment</h4>
            <video class="r-stretch" controls onloadstart="this.playbackRate = 10;">
                <source data-src="http://www.iri.upc.edu/groups/perception/dual_arm_planning/video/itm_demo_real.mp4" type="video/mp4">
            </video>
            <aside class="notes">
                Here's a short video showing a qualitative experiment, and mostly what I want you to notice here
                is how the robot that fetches the pieces avoids collisions with the sphere robot without moving
                it to its home position.
            </aside>
        </section>

        <section>
            <h3 class="numbered-section">Planning in Face of Stochastic Outcomes</h3>


            <div class="r-hstack">

                <div style="width:50%">
                    <img src="img/contributions/splash12.svg">
                </div>

                <div style="width:50%;" class="fragment">
                    <figure style="">
                      <img width="200" src="img/handling-uncertain-outcomes/unscrew.gif">
                      <img width="200" src="img/handling-uncertain-outcomes/remove-pcb.gif">
                      <figcaption>Disassembly with uncertainty</figcaption>
                    </figure>

                </div>

            </div>


            <div class="reference" style="padding-top: 50px;" >
                <strong>A. Suárez-Hernández</strong>, C. Torras, and G. Alenyà.
                  "Practical resolution methods for MDPs in robotics exemplified
                  with disassembly planning." In <strong>IEEE RA-L (2019)</strong>, pp. 2282–2288.
            </div>

            <aside class="notes">
                <p>
                In this contribution we shift our focus now to action with more
                than one possible outcome. And now, the manipulation use case
                is disassembling electromechanical devices, which again,
                shows several problems that are general to robotics.
                </p>
            </aside>

        </section>

        <section>
            <h4>Addressed problems</h4>

            <div class="r-hstack">

                <div style="width:50%;">

                    <ul>
                        <li class="fragment semi-fade-out" data-fragment-index="1">Handle uncertain outcomes (<strong>O2</strong>)</li>
                        <li class="fragment" data-fragment-index="1">Handle partial observability (occlusions) (<strong>O3</strong>)</li>
                    </ul>

                </div>

                <div class="r-stack" style="width:50%">
                    <figure class="fragment fade-out" data-fragment-index="1">
                      <img style="padding-bottom:200px;" src="img/handling-uncertain-outcomes/stochastic-action.png">
                      <img style="height:200px;position:fixed;left:620px;top:257px;" src="img/handling-uncertain-outcomes/successful-action.gif">
                      <img style="height:200px;position:fixed;left:941px;top:257px;" src="img/handling-uncertain-outcomes/unsuccessful-action.gif">
                    </figure>
                    <img class="fragment" data-fragment-index="1" style="height:375px;" src="img/handling-uncertain-outcomes/device-with-hidden-components.png">
                </div>
            </div>

            <aside class="notes">
                <p>
                What are the problems we face? I've already advanced that one
                of them is actions with uncertain effects, and we model this
                probabilistically. In the most simple terms, this can means that
                some actions may fail, and that some can even result in a dead-end,
                like breaking a tool.
                </p>
                <p>
                And also, it's very frequent that some components occlude others,
                so we want to handle that as well.
                </p>
            </aside>

        </section>

        <section>
            <h4>Literature</h4>

            <div class="r-stack">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/handling-uncertain-outcomes/literature1.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/handling-uncertain-outcomes/literature2.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/handling-uncertain-outcomes/literature3.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/handling-uncertain-outcomes/literature4.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/handling-uncertain-outcomes/literature5.svg">
            </div>

            <aside class="notes">
                <p>The first work I'd like to mention is FF-Replan, one of the
                earliest attempts at tackling stochastic planning problems. It had
                the problem that it was way too optimistic, and it underperformed
                is many interesting planning domains.</p>

                <p>Then we have more sophisticated planning algorithms like
                Labeled Real Time Dynamic Programming, which is based on the
                Real Time A* algorithm. This is one of the most known algorithms
                for stochastic planning, together with Monte Carlo Tree Search,
                and we will compare our method to this one.</p>

                <p>Then we have approaches specific to disassembly. This work in
                particular fails to acknowledge outcome stochasticity and components
                hidden by occlusions
                </p>

                <p>
                More recently, we have the Liam robot, by Apple, which implements
                a step-by-step algorithm rather, so it falls at the left hand
                side of the spectrum of solutions I showed you earlier.
                </p>

                <p>And we also have this work, which acknowledges hidden components,
                but does not plan for uncertain outcomes.</p>
            </aside>
        </section>

        <section data-auto-animate>
            <h4>Specific contributions</h4>

            <div style="width:100%;height:700px;position:absolute">
            <img style="width:500px;" src="img/handling-uncertain-outcomes/architecture.png">
            </div>

            <!--<div class="r-hstack" style="justify-content:space-evenly;">-->
              <!--<figure class="fragment" style="width:50%;vertical-align:bottom;">-->
                <!--<img style="height:400px;" src="img/handling-uncertain-outcomes/architecture.svg">-->
                <!--<figcaption>System architecture</figcaption>-->
              <!--</figure>-->
              <!--<figure style="width:50%;vertical-align:bottom;" class="">-->
                <!--<img class="fragment" src="img/handling-uncertain-outcomes/subtask-selection.png">-->
              <!--</figure>-->
            <!--</div>-->
            <aside class="notes">
                <p>This is our proposed decision unit. The critical contributions
                here are the subtask selector module, which performs dependency
                analysis and extracts smaller tasks that can be solved independently
                from each other, and the determinization module, that takes a problem,
                a Markov Decision Process, and produces a deterministic planning problem
                encoding the transition probabilities as costs.</p>
            </aside>

        </section>

        <section data-auto-animate>
            <h4>Specific contributions</h4>

            <div style="width:100%;height:700px;position:absolute">
            <img style="height:200px;max-height:none;position:absolute;top:0px;left:0px;" src="img/handling-uncertain-outcomes/architecture.png">
            <div class="fragment" data-id="box" data-fragment-index="1" style="border:solid red;width:120px;height:30px;position:absolute;top:69px;left:44px;"></div>
            <div class="r-stack">
                <img class="fragment fade-in-then-out" style="width:700px;position:absolute;top:50%;left:50%;transform: translate(-50%,-50%);" data-fragment-index="1" src="img/handling-uncertain-outcomes/subtask-selection-simple.svg">
                <img class="fragment" style="width:800px;position:absolute;top:50%;left:50%;transform: translate(-50%,-50%);" data-fragment-index="2" src="img/handling-uncertain-outcomes/subtask-selection.svg">
            </div>
            </div>

            <aside class="notes">
                <p>
                Let's go to the first of these two. Here's a pictoric representation
                of what it does: it identifies individual tasks, computes precedence
                relationships among them based on occlusions and then extract
                the tasks with no topological restrictions. Here's a more
                real example, also with components that are completely hidden
                and that the planner knows nothing about, shown in gray.
                You can see here that, instead
                of having one big POMDP, we get the smaller MDPs at the right
                that can be solved individually, and the partial observability
                can be handled with periodic rotations of the feasible tasks.
                </p>
            </aside>
        </section>

        <section data-auto-animate>
            <h4>Specific contributions</h4>

            <div style="width:100%;height:700px;position:absolute">
            <img style="height:200px;max-height:none;position:absolute;top:0px;left:0px;" src="img/handling-uncertain-outcomes/architecture.png">
            <div data-id="box" style="border:solid red;width:120px;height:30px;position:absolute;top:129px;left:44px;"></div>
            <img class="" style="width:800px;position:absolute;top:50%;left:50%;transform: translate(-50%,-50%);" data-fragment-index="1" src="img/handling-uncertain-outcomes/determinization.svg">
            </div>

            <aside class="notes">
                <p>
                Now, determinization consists on transforming a probabilistic
                problem into a deterministic one. There are several ways of doing this,
                and the one we've picked is call alpha cost likelihood. What this
                determinization strategy does is introduce a new action for each outcome,
                as if the outcomes were conscious decisions made by the agent
                and it assigns each outcome a cost that is higher the lower the probability
                gets. There's another component to this cost which is the original cost
                of the action, and this cost is weighted by a parameter alpha.
                What this allows us to do is balance eagerness vs safety: if the alpha
                is 0 we only take into account the probabilities, and if it's higher
                we put more weight in the original cost.

                Now, while I did not invent alpha cost likelihood, I must say it's
                been barely used in the literature, with only a couple of papers or
                three that use it. So I wanted to bring more attention to this technique
                that I think is superior to others like All-Outcome Determinization
                or Single Outcome Determinization, and also in my work I put more
                effort on analyzing this balance between risk and safety.
                </p>
            </aside>
        </section>

        <section>
            <h4>Results</h4>

            <div class="r-stack">
                <div class="r-hstack" data-fragment-index="1">
                    <figure>
                    <img src="img/handling-uncertain-outcomes/results_success.svg">
                    <figcaption>Success ratio</figcaption>
                    </figure>
                    <figure class="fragment">
                    <img src="img/handling-uncertain-outcomes/results_cost.svg">
                    <figcaption>Cost (#actions)</figcaption>
                    </figure>
                </div>
            </div>

        </section>

        <section>

            <h3 class="numbered-section">Leveraging Simulators to Minimize Risk</h3>


            <div class="r-hstack">

                <div style="width:50%">
                    <img src="img/contributions/splash13.svg">
                </div>

                <div style="width:50%;" class="fragment">
                    <figure style="">
                      <img width="300" src="img/leveraging-simulators/lever-failure1.gif">
                      <img width="300" src="img/leveraging-simulators/lever-failure2.gif">
                      <figcaption>How to anticipate failures?</figcaption>
                    </figure>

                </div>

            </div>


            <div class="reference" style="padding-top: 50px;" >
                <strong>A. Suárez-Hernández</strong>, T. Gaugry, J. Segovia-Aguas,
                  A. Bernardin, C. Torras, M. Marchal, and G. Alenyà.
                  "Leveraging Multiple Environments for Learning and Decision Making:
                  a Dismantling Use Case.” In: <strong>IEEE/RSJ IROS 2020</strong>, pp. 6902–6908.
            </div>

        </section>

        <section>
            <h4>Problems addressed</h4>


            <div class="r-hstack">

                <div style="width:50%;">

                    <ul>
                        <li class="fragment semi-fade-out" data-fragment-index="1">Unknown frequency of outcomes (<strong>O2</strong>)</li>
                        <li class="fragment" data-fragment-index="1">Distinguish among actions? (<strong>O1</strong>)</li>
                    </ul>

                </div>

                <div class="r-stack" style="width:50%">
                    <figure class="fragment fade-out" data-fragment-index="1">
                        <img style="width:40%;" src="img/leveraging-simulators/exp1.gif">
                        <img style="width:40%;" src="img/leveraging-simulators/exp2.gif">
                        <img style="width:40%;" src="img/leveraging-simulators/exp3.gif">
                    </figure>
                    <figure class="fragment" data-fragment-index="1">
                        <img style="width:75%" src="img/leveraging-simulators/lever-success.gif">
                        <img style="width:75%" src="img/leveraging-simulators/lever-success2.gif">
                    </figure>
                </div>
            </div>


        </section>

        <section>
            <h4>Literature<h4>
            <div class="r-stack">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/leveraging-simulators/literature1.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/leveraging-simulators/literature2.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/leveraging-simulators/literature3.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/leveraging-simulators/literature4.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/leveraging-simulators/literature5.svg">
            </div>
        </section>

        <section>
            <h4>Specific contributions</h4>

            <div class="r-hstack spaced-horizontally">
                <figure style="width:50%" class="fragment">
                    <img src="img/leveraging-simulators/menid.svg">
                    <figcaption>Multiple Environment NID (MENID) rules</figcaption>
                </figure>

                <figure style="width:50%;" class="fragment">
                    <img style="width:50%;" src="img/leveraging-simulators/architecture.png">
                    <img style="position:fixed;width:100px;top:405px;left:776px;" src="img/leveraging-simulators/lever-success-sim.gif">
                    <img style="position:fixed;width:100px;top:405px;left:932px;" src="img/leveraging-simulators/lever-success.gif">
                    <figcaption style="padding-top:100px">System architecture</figcaption>
                </figure>

            </div>

        </section>

        <section>
            <h4>Model resolution</h4>

            <figure>
                <img src="img/leveraging-simulators/simulation-quality.svg">
                <figcaption>Effect of model resolution on time and accuracy</figcaption>
            </figure>
        </section>

        <section>
            <h4>Algorithm visualization (test+target)</h4>

            <video class="r-stretch" controls>
                <source data-src="video/bandits_w_sim.mp4" type="video/mp4">
            </video>

        </section>

        <!--<section>-->
            <!--<h4>Algorithm visualization (only target)</h4>-->

            <!--<video class="r-stretch" controls>-->
                <!--<source data-src="video/bandits_wo_sim.mp4" type="video/mp4">-->
            <!--</video>-->

        <!--</section>-->

        <section data-auto-animate>
            <h4>Results</h4>

            <div data-id="plots" class="r-vstack">
                <figure>
                    <img src="img/leveraging-simulators/results-simulation.png">
                    <figcaption data-id="caption1">Results low-accuracy simulation $ \rightarrow $ high-accuracy simulation</figcaption>
                </figure>
                <figure style="padding-top:10px;">
                    <img src="img/leveraging-simulators/results-robot.png">
                    <figcaption data-id="caption2">Results simulation $ \rightarrow $ real robot</figcaption>
                </figure>
            </div>

            <div class="fragment" data-id="box" style="position:absolute;border:red solid;top:73px;left:30px;width:1115px;height:300px">
            </div>
        </section>

        <section data-auto-animate>
            <h4>Results</h4>

            <div data-id="plots" class="r-vstack">
                <figure>
                    <img src="img/leveraging-simulators/results-simulation.png">
                    <figcaption data-id="caption1">Results low-accuracy simulation $ \rightarrow $ high-accuracy simulation</figcaption>
                </figure>
                <figure style="padding-top:10px;">
                    <img src="img/leveraging-simulators/results-robot.png">
                    <figcaption data-id="caption2">Results simulation $ \rightarrow $ real robot</figcaption>
                </figure>
            </div>

            <div data-id="box" style="position:absolute;border:red solid;top:407px;left:30px;width:1115px;height:300px">
            </div>
        </section>

        <section data-auto-animate>
            <h4>Results</h4>

            <div data-id="plots" class="r-vstack">
                <figure>
                    <img src="img/leveraging-simulators/results-simulation.png">
                    <figcaption data-id="caption1">Results low-accuracy simulation $ \rightarrow $ high-accuracy simulation</figcaption>
                </figure>
                <figure style="padding-top:10px;">
                    <img src="img/leveraging-simulators/results-robot.png">
                    <figcaption data-id="caption2">Results simulation $ \rightarrow $ real robot</figcaption>
                </figure>
            </div>

            <div data-id="box" style="position:absolute;border:red solid;top:73px;left:30px;width:400px;height:630px;">
            </div>
        </section>

        <section data-auto-animate>
            <h4>Results</h4>

            <div data-id="plots" class="r-vstack">
                <figure>
                    <img src="img/leveraging-simulators/results-simulation.png">
                    <figcaption data-id="caption1">Results low-accuracy simulation $ \rightarrow $ high-accuracy simulation</figcaption>
                </figure>
                <figure style="padding-top:10px;">
                    <img src="img/leveraging-simulators/results-robot.png">
                    <figcaption data-id="caption2">Results simulation $ \rightarrow $ real robot</figcaption>
                </figure>
            </div>

            <div data-id="box" style="position:absolute;border:red solid;top:73px;left:406px;width:400px;height:630px;">
            </div>
        </section>

        <section data-auto-animate>
            <h4>Results</h4>

            <div data-id="plots" class="r-vstack">
                <figure>
                    <img src="img/leveraging-simulators/results-simulation.png">
                    <figcaption data-id="caption1">Results low-accuracy simulation $ \rightarrow $ high-accuracy simulation</figcaption>
                </figure>
                <figure style="padding-top:10px;">
                    <img src="img/leveraging-simulators/results-robot.png">
                    <figcaption data-id="caption2">Results simulation $ \rightarrow $ real robot</figcaption>
                </figure>
            </div>

            <div data-id="box" style="position:absolute;border:red solid;top:73px;left:786px;width:400px;height:630px;">
            </div>
        </section>

        <section>
            <h2 class="numbered-section">Learning Planning Operators</h2>

            <div class="r-stack">
                <img class="r-stretch" src="img/contributions/splash2.svg">
                <figure class="fragment" style="padding:10px;position:fixed;left:0px;top:120px;background-color:white;border: black dashed">
                    <img style="height:500px;" src="img/contributions/projects2.png">
                    <figcaption>Intuitive programming of SAR</figcaption>
                </figure>
            </div>


            <!--<img class="r-stretch" src="img/contributions/splash2.svg">-->

        </section>

        <section>
            <h3 class="numbered-section">STRIPS Action Discovery</h3>

            <div class="r-hstack">

                <div style="width:50%">
                    <img src="img/contributions/splash24.svg">
                </div>

                <div style="width:50%;" class="fragment">
                    <figure style="">
                      <img style="width:90%;" src="img/udam/udam-teaser.png">
                      <figcaption>Example STRIPS action for visit-all domain</figcaption>
                    </figure>

                </div>

            </div>

            <div class="reference" style="padding-top: 100px;">
                <strong>A. Suárez-Hernández</strong>, J. Segovia-Aguas, C. Torras, and
                G. Alenyà. “STRIPS action discovery.” In: arXiv preprint arXiv:
                2001.11457 (2020). <strong>Presented in the 1st Workshop on Generalization in
                Planning at the 2020 AAAI conference</strong>.
            </div>
        </section>

        <section>
            <h4>Addressed problems</h4>

            <div class="r-hstack">
                <ul>
                    <li class="fragment semi-fade-out" data-fragment-index="1">Learning STRIPS state traces (<strong>O4</strong>)</li>
                    <li class="fragment" data-fragment-index="1"><strong>Minimal information</strong></li>
                </ul>

                <div class="r-stack">
                    <img class=""src="img/udam/traces1.svg">
                    <img class="fragment" data-fragment-index="1" src="img/udam/traces2.svg">
                    <img class="fragment"src="img/udam/traces3.svg">
                </div> 
            </div>
        </section>

        <section>
            <h4>Literature</h4>

            <div class="r-stack">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/udam/literature1.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/udam/literature2.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/udam/literature3.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/udam/literature4.svg">
            </div>
        </section>

        <section>
            <h4>FAMA/DAM compilation</h4>

            <p>Learn action models for a given configuration (known #actions, #parameters per action)</p>

            <div class="r-hstack spaced-horizontally">
                <figure>
                <img class="" style="height:225px;" src="img/udam/strips.png">
                <figcaption>STRIPS action</figcaption>
                </figure>
                <figure class="fragment">
                <img class="" style="height:225px;" src="img/udam/fama.png">
                <figcaption>Learn via classical planning</figcaption>
                </figure>
            </div>

        </section>

        <section>
            <h4>Specific contributions</h4>

            <figure>
                <div class="r-stack">
                    <img class="" style="width:800px;" src="img/udam/udam1.svg">
                    <img class="fragment" style="width:800px;" src="img/udam/udam2.svg">
                </div>
                <figcaption>Unsupervised Discovery of Action Models</figcaption>
            </figure>
        </section>

        <section>
            <h4>Results</h4>

            <img src="img/udam/results.png">
        </section>

        <section>
            <h3 class="numbered-section">INtuitive PROgramming</h3>

            <div class="r-hstack">

                <div style="width:50%">
                    <img src="img/contributions/splash25.svg">
                </div>

                <div style="width:50%;" class="fragment">
                    <figure style="">
                      <img style="width:60%;" src="img/inpro/aleks.png">
                      <figcaption>A teacher demonstrating a cognitive exercise</figcaption>
                    </figure>

                </div>

            </div>

            <div class="reference" style="padding-top: 100px;">
                A. Andriella, <strong>A. Suárez-Hernández</strong>, J. Segovia-Aguas, 
                C. Torras, and G, Alenya. "Natural teaching of robot-assisted
                rearranging exercises for cognitive training." In <strong>ICSR 2019</strong>, pp. 611–621
            </div>
            <div class="reference" style="padding-top: 10px;">
                <strong>A. Suárez-Hernández</strong>, A. Andriella, A. Taranović,
                J. Segovia-Aguas, C. Torras, and G. Alenyà. "Automatic
                learning of cognitive exercises for socially assistive robotics."
                In <strong>RO-MAN 2021</strong>, pp. 139–146
            </div>
        </section>

        <section>
            <h4>Addressed problems</h4>

            <div class="r-hstack">
                <ul>
                    <li class="fragment" data-fragment-index="1">How can non-expert users extend robot's behavior (<strong>O4</strong>)?</li>
                </ul>

                <figure>
                     <img style="height:300px;" src="img/inpro/games.png">
                     <figcaption>Cognitive exercises set-up</figcaption>
                </figure>

            </div>
 
        </section>

        <section>
            <h4>Literature</h4>

            <div class="r-stack">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/inpro/literature1.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/inpro/literature2.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/inpro/literature3.svg">
                <img class="r-stretch fragment fade-in-then-semi-out" src="img/inpro/literature4.svg">
            </div>
        </section>

        <section>
            <h4>Specific contributions</h4>
            
            <div class="r-hstack">

                <div style="width:50%;">

                    <ul>
                        <li class="fragment semi-fade-out" data-fragment-index="1"><strong>IN</strong>tuitive <strong>PRO</strong>gramming (INPRO): practical application of action model learning algorithm (INPRO)</li>
                        <li class="fragment" data-fragment-index="1">Feedback strategies to inform teacher</li>
                    </ul>

                </div>

                <div class="r-stack" style="width:50%">
                    <figure class="fragment" data-fragment-index="1">
                        <img style="width:75%" src="img/inpro/screen.png">
                        <figcaption>INPRO feedback</figcaption>
                    </figure>
                </div>
            </div>

        </section>

        <section>
            <h4>Hypotheses</h4>

            <ul>
                <li><strong>H1</strong>: teachers from the minimum feedback group will require more traces to
                    complete the exercises</li>
                <li><strong>H2</strong>: more teachers in the full feedback group will teach all exercise rules</li>
            </ul>
        </section>

        <section>
            <h4>Results</h4>

            <div class="r-hstack">
              <figure style="width:50%;" class="">
                <img style="height:150px;" src="img/inpro/table.png">
                <figcaption>User study results (tabular)</figcaption>
              </figure>
              <figure class="fragment" style="width:50%;vertical-align:bottom;">
                <img style="height:370px;" src="img/inpro/plot.png">
                <figcaption>Error vs proportion of participants</figcaption>
              </figure>
            </div>
        </section>

        <section>
            <h3 class="numbered-section">Online Action Recognition through Unification</h3>

            <div class="r-hstack">

                <div style="width:50%">
                    <img src="img/contributions/splash26.svg">
                </div>

                <div style="width:50%;" class="fragment">
                    <figure style="">
                      <img style="height:300px;" src="img/oaru/sokoban_animation.gif">
                      <figcaption>Could we learn Sokoban rules from demonstrations?</figcaption>
                    </figure>

                </div>

            </div>


            <div class="reference" style="padding-top: 50px;">
                <strong>A. Suárez-Hernández</strong>, J. Segovia-Aguas, C. Torras, and
                G. Alenyà. "Online Action Recognition." In: <strong>AAAI 2021 Conference</strong>,
                pp. 11981–11989
            </div>
        </section>

        <section>
            <h4>Addressed problems</h4>

            <ul>
                <li class="fragment">Real-time STRIPS action learning</li>
                <li class="fragment">Learning even with Partial Observability</li>
                <li class="fragment">Few-shot performance</li>
            </ul>
        </section>

        <section>
            <h4>Specific contributions</h4>

            <div class="r-hstack">

                <div style="width:50%;">

                    <ul>
                        <li class="fragment semi-fade-out" data-fragment-index="1">New algorithm for merging actions (AU)</li>
                        <li class="fragment" data-fragment-index="1">Online Action through Unification (OARU)</li>
                    </ul>

                </div>

                <div class="r-stack" style="width:50%">
                    <img style="height:600px;" src="img/oaru/teaser.svg">
                </div>
            </div>
        </section>

        <section>


            <video id="itm-video" controls onloadstart="this.playbackRate = 1;">
                <source data-src="video/teaser-oaru.mp4" type="video/mp4">
            </video>

        </section>

        <section>
            <h4>Results (full observability)</h4>

            <div class="r-hstack">
              <figure style="width:50%;" class="">
                <img style="height:300px;" src="img/oaru/results_a.png">
                <figcaption>Statistics</figcaption>
              </figure>
              <figure class="fragment" style="width:50%;vertical-align:bottom;">
                <img style="height:370px;" src="img/oaru/full_obs_updates-crop.svg">
                <figcaption>#Updates vs #steps</figcaption>
              </figure>
            </div>

        </section>

        <section>
            <h4>Results (partial observability)</h4>

            <div class="r-hstack">
              <figure style="width:50%;" class="">
                <img style="height:300px;" src="img/oaru/results_b.png">
                <figcaption>Statistics</figcaption>
              </figure>
              <figure class="fragment" style="width:50%;vertical-align:bottom;">
                <img style="height:370px;" src="img/oaru/partial_obs_updates-crop.svg">
                <figcaption>#Updates vs #steps</figcaption>
              </figure>
            </div>
        </section>

        <section>
            <h3 class="numbered-section">INtuitive PROgramming 2</h3>

            <div class="r-hstack">

                <div style="width:50%">
                    <img src="img/contributions/splash27.svg">
                </div>

                <div style="width:50%;" class="fragment">
                    <figure style="">
                      <img style="height:300px;" src="img/inpro2/teaching-helena.gif">
                      <figcaption>Teaching cognitive exercises to a robot</figcaption>
                    </figure>

                </div>

            </div> 

            <div class="reference" style="padding-top: 100px;">
                A. Suárez-Hernández, A. Andriella, C. Torras,
                and G. Alenyà. "User Interactions and Negative Examples
                to Improve the Learning of Semantic Rules in a Cognitive
                Exercise Scenario." In IROS 2023, pp. 7953–7960.
            </div>
        </section>

        <section>
            <h4>Addressed problems</h4>

            <ul>
                <li class="fragment fade-in-then-semi-out" data-fragment-index="1">Learn more complex exercises</li>
                <li class="fragment" data-fragment-index="2">Until now: robot hasn't been very proactive</li>
                <li class="fragment">Learn goals?</li>
            </ul>
        </section>

        <section>
            <h4>Specific contributions</h4>

            <ul>
                <li class="fragment semi-fade-out" data-fragment-index="1">Adopt OARU in SAR scenario</li>
                <li class="fragment fade-in-then-semi-out" data-fragment-index="1">Improve OARU with negative examples</li>
                <li class="fragment fade-in-then-semi-out">Learn goals</li>
                <li class="fragment fade-in-then-semi-out">Proactive questioning</li>
            </ul>
        </section>

        <section>
            <h4>Execution example</h4>

            <div class="r-stack">
                <img class="r-stretch" src="img/inpro2/method1_frame1.svg">
                <img class="r-stretch fragment" src="img/inpro2/method1_frame2.svg">
                <img class="r-stretch fragment" src="img/inpro2/method1_frame3.svg">
                <img class="r-stretch fragment" src="img/inpro2/method1_frame4.svg">
                <img class="r-stretch fragment" src="img/inpro2/method1_frame5.svg">
                <img class="r-stretch fragment" src="img/inpro2/method1_frame6.svg">
                <img class="r-stretch fragment" src="img/inpro2/method1_frame7.svg">
                <img class="r-stretch fragment" src="img/inpro2/method1_frame8.svg">
                <img class="r-stretch fragment" src="img/inpro2/method1_frame9.svg">
                <img class="r-stretch fragment" src="img/inpro2/method1_frame10.svg">
            </div>

        </section>

        <section>
            <h4>Prompting criterion</h4>

            <div class="r-stack">
                <img class="r-stretch" src="img/inpro2/method2_frame1.svg">
                <img class="r-stretch fragment" src="img/inpro2/method2_frame2.svg">
                <img class="r-stretch fragment" src="img/inpro2/method2_frame3.svg">
                <img class="r-stretch fragment" src="img/inpro2/method2_frame4.svg">
                <img class="r-stretch fragment" src="img/inpro2/method2_frame5.svg">
                <img class="r-stretch fragment" src="img/inpro2/method2_frame6.svg">
                <img class="r-stretch fragment" src="img/inpro2/method2_frame7.svg">
            </div>

        </section>

        <section>
            <h4>Quantitative results</h4>

            <img class="r-stretch" src="img/inpro2/results.svg">
        </section>

        <section>
            <h4>Qualitative results</h4>

            <video class="r-stretch" loop controls>
                <source data-src="video/inpro2.mp4" type="video/mp4">
            </video>
        </section>

        <section data-auto-animate>
            <h2 class="numbered-section">Conclusions</h2>

            <p class="r-frame fragment" style="font-size:75%;" data-fragment-index="1">New Methods for
            <span class="fragment highlight-blue" data-fragment-index="2" data-id="item1">Bridging Symbolic-Geometric Reasoning</span>,
            <span class="fragment highlight-red" data-fragment-index="2" data-id="item2">Addressing Uncertainty</span> and
            <span class="fragment highlight-green" data-fragment-index="2" data-id="item3">Action Learning</span>
            in Task Planning for Robotics</p>

            <!--<p class="fragment" data-fragment-index="2">Key advances:</p>-->
            <!--<ul class="fragment" data-fragment-index="2">-->
                <!--<li class="">Simplification <span class="fragment">$ \rightarrow $</span> <span class="fragment blue-box"></span> <span class="fragment red-box"></span></li>-->
                <!--<li class="">Beyond-symbolic reasoning <span class="fragment">$ \rightarrow $</span> <span class="fragment blue-box"></span> </li>-->
                <!--<li class="">Hands-off learning <span class="fragment">$ \rightarrow $</span> <span class="fragment green-box"></span> </li>-->
            <!--</ul>-->

        </section>

        <section data-auto-animate>
            <h2 class="numbered-section dont-increment">Conclusions</h2>

            <div class="r-hstack spaced-horizontally" style="font-size:75%;">
            <span data-id="item1" style="color:#1b91ff;">Bridging Symbolic-Geometric Reasoning</span>
            <span data-id="item2" style="color:#ff2c2d;">Addressing Uncertainty</span>
            <span data-id="item3" style="color:#17ff2e;">Action Learning</span>
            </div>


            <p class="fragment" data-fragment-index="2">Key advances:</p>
            <ul class="fragment" data-fragment-index="2">
                <li data-id="simplification" class="">Simplification <span class="fragment" data-fragment-index="3">$ \rightarrow $</span> <span class="fragment blue-box" data-fragment-index="3"></span> <span data-fragment-index="3" class="fragment red-box"></span></li>
                <li data-id="beyond" class="">Beyond-symbolic reasoning <span data-fragment-index="4" class="fragment">$ \rightarrow $</span> <span data-fragment-index="4" class="fragment blue-box"></span> </li>
                <li data-id="handsoff" class="">Hands-off learning <span data-fragment-index="5" class="fragment">$ \rightarrow $</span> <span data-fragment-index="5" class="fragment green-box"></span> </li>
            </ul>

        </section>

        <section data-auto-animate>
            <h2 class="numbered-section dont-increment">Conclusions</h2>

            <div class="r-hstack spaced-horizontally" style="font-size:75%;">
            <span data-id="item1" style="color:#1b91ff;">Bridging Symbolic-Geometric Reasoning</span>
            <span data-id="item2" style="color:#ff2c2d;">Addressing Uncertainty</span>
            <span data-id="item3" style="color:#17ff2e;">Action Learning</span>
            </div>


            <p class="">Key advances:</p>
            <ul class="">
                <li data-id="simplification" class="">
                    Simplification <span class="">$ \rightarrow $</span> <span class="blue-box" ></span> <span class="red-box"></span>
                    <ul>
                        <li>Hierarchical representations</li>
                        <li>Determinization</li>
                    </ul>
                </li>
                <li data-id="beyond" class="">Beyond-symbolic reasoning <span class="">$ \rightarrow $</span> <span class="blue-box"></span> </li>
                <li data-id="handsoff" class="">Hands-off learning <span class="">$ \rightarrow $</span> <span class="green-box"></span> </li>
            </ul>

        </section>

        <section data-auto-animate>
            <h2 class="numbered-section dont-increment">Conclusions</h2>

            <div class="r-hstack spaced-horizontally" style="font-size:75%;">
            <span data-id="item1" style="color:#1b91ff;">Bridging Symbolic-Geometric Reasoning</span>
            <span data-id="item2" style="color:#ff2c2d;">Addressing Uncertainty</span>
            <span data-id="item3" style="color:#17ff2e;">Action Learning</span>
            </div>


            <p class="">Key advances:</p>
            <ul class="">
                <li data-id="simplification" class="">
                    Simplification <span class="">$ \rightarrow $</span> <span class="blue-box" ></span> <span class="red-box"></span>
                </li>
                <li data-id="beyond" class="">
                    Beyond-symbolic reasoning <span class="">$ \rightarrow $</span> <span class="blue-box"></span>
                    <ul>
                        <li>Uncertainty checks and collision logic in HTNs</li>
                        <li>Leveraging simulators</li>
                    </ul>
                </li>
                <li data-id="handsoff" class="">
                    Hands-off learning <span class="">$ \rightarrow $</span> <span class="green-box"></span>
                </li>
            </ul>

        </section>

        <section data-auto-animate>
            <h2 class="numbered-section dont-increment">Conclusions</h2>

            <div class="r-hstack spaced-horizontally" style="font-size:75%;">
            <span data-id="item1" style="color:#1b91ff;">Bridging Symbolic-Geometric Reasoning</span>
            <span data-id="item2" style="color:#ff2c2d;">Addressing Uncertainty</span>
            <span data-id="item3" style="color:#17ff2e;">Action Learning</span>
            </div>


            <p class="">Key advances:</p>
            <ul class="">
                <li data-id="simplification" class="">
                    Simplification <span class="">$ \rightarrow $</span> <span class="blue-box" ></span> <span class="red-box"></span>
                </li>
                <li data-id="beyond" class="">
                    Beyond-symbolic reasoning <span class="">$ \rightarrow $</span> <span class="blue-box"></span>
                </li>
                <li data-id="handsoff" class="">
                    Hands-off learning <span class="">$ \rightarrow $</span> <span class="green-box"></span>
                    <ul>
                        <li>Learning without action signatures</li>
                        <li>Practical implementation in SAR</li>
                    </ul>
                </li>
            </ul>

        </section>

        <section>
            <h2 class="numbered-section">Future Work</h2>

            <ul>
                <li class="">Simplification
                    <ul>
                        <li>Further strategies to decompose tasks</li>
                        <li>New determinization algorithms</li>
                    </ul>
                </li>

                <li class="fragment">Beyond symbolic reasoning
                    <ul>
                        <li>Leverage more than two environments</li>
                        <li>Compare interleaved simulation/real execution against transfer learning</li>
                    </ul>
                </li>

                <li class="fragment">Hands-off learning
                    <ul>
                        <li>Improve prompting strategies (INPRO 2)</li>
                        <li>Combine OARU/INPRO2 with learned embeddings</li>
                    </ul>
                </li>


            </ul>

        </section>

        <section>
            <h2 style="padding-top:12px;" class="r-fit-text">New Methods for Bridging Symbolic-Geometric Reasoning, Addressing Uncertainty and Action Learning in Task Planning for Robotics</h2>


            <div class="r-stack">
                <img class="r-stretch" src="img/contributions/frame1.svg">
                <img class="r-stretch" src="img/contributions/frame2.svg">
            </div>
        </section>

      </div>
      <!--<div class="frame bottom"></div>-->
      <div class="frame top"></div>
      <!--<div class="frame left"></div>-->
      <!--<div class="frame right"></div>-->
      <div class="logos">
          <img src="assets/logos/IRI_logo.svg" height="50px"/>
          <img src="assets/logos/UPC_logo.svg" height="50px"/>
      </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/dist/reveal.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/plugin/notes/notes.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/plugin/math/math.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.1.0/plugin/zoom/zoom.min.js"></script>

    <!--<script src="reveal.js/dist/reveal.js"></script>-->
    <!--<script src="reveal.js/plugin/notes/notes.js"></script>-->
    <!--<script src="reveal.js/plugin/markdown/markdown.js"></script>-->
    <!--[><script src="reveal.js/plugin/highlight/highlight.js"></script><]-->
    <!--<script src="reveal.js/plugin/math/math.js"></script>-->

    <!-- D3 Graphviz -->
    <!--<script src="https://d3js.org/d3.v5.min.js"></script>-->
    <!--<script src="https://unpkg.com/@hpcc-js/wasm@0.3.11/dist/index.min.js" type="javascript/worker"></script>-->
    <!--<script src="https://unpkg.com/d3-graphviz@3.0.5/build/d3-graphviz.js"></script>-->

    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        autoPlayMedia: true,
        hash: true,
        slideNumber: "c/t",
        margin: 0.08,
        width: 1200,
        controls: true,
        //keyboard: {
        //  84: renderGraph
        //},
        math: {
          mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },
        // Learn about plugins: https://revealjs.com/plugins/
        //plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ]
        plugins: [ RevealMarkdown, RevealZoom, RevealNotes, RevealMath ]
      })
      .then( () => {
        var headerNumber = [0,0,0,0,0,0],
            headers = document.querySelectorAll("h1.numbered-section, h2.numbered-section, h3.numbered-section, h4.numbered-section, h5.numbered-section, h6.numbered-section")
        for (var h of headers) {
          var level = Number(h.tagName[1]) - 1;
          if (!h.classList.contains("dont-increment")) {
            ++headerNumber[level];
            for (var inner_level = level+1; inner_level < 6; ++inner_level)
              headerNumber[inner_level] = 0;
          }
          var spanElement = document.createElement("SPAN");
          var spanText = document.createTextNode(headerNumber.slice(1,level+1).join(".")+". ");
          spanElement.classList.add("numeral");
          spanElement.appendChild(spanText);
          h.insertBefore(spanElement, h.firstChild);
        }
      })
    </script>
  </body>
</html>
